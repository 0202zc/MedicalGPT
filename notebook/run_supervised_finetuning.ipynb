{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "| Stage 2: Supervised Fine-tuning | 有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图   | [scripts/supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/scripts/supervised_finetuning.py) | [scripts/run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/scripts/run_sft.sh)  | [notebook/run_supervised_finetuning.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/notebook/run_supervised_finetuning.ipynb)     | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_supervised_finetuning.ipynb) |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 2: Supervised FineTuning\n",
    "\n",
    "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置运行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
    "\n",
    "# !git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "# %cd MedicalGPT\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装库和依赖包：\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers>=4.28.1\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm>=4.47.0\n",
    "peft>=0.3.0\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 咱们开始吧\n",
    "\n",
    "环境配置完成，开始导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:42:46.393028: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 17:42:46.567745: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-07 17:42:48.291399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 17:42:48.291621: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 17:42:48.291632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('disk/nlp/MedicalGPT/notebook/run_supervised_finetuning.ipynb')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 XuMing(xuming624@qq.com) and The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n",
    "\n",
    "part of this code is adapted from https://github.com/shibing624/textgen/blob/main/textgen/llama/llama_model.py\n",
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_int8_training\n",
    "from transformers import (\n",
    "    BloomForCausalLM,\n",
    "    AutoModel,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    BloomTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.trainer import TRAINING_ARGS_NAME\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n",
    "    \"chatglm\": (AutoModel, AutoTokenizer),\n",
    "    \"llama\": (LlamaForCausalLM, LlamaTokenizer),\n",
    "}\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belle_open_source_1k.json\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_type: str = field(\n",
    "        default='bloom',\n",
    "        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n",
    "    )\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"bigscience/bloomz-560m\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=\"float16\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    device_map: Optional[str] = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_type is None:\n",
    "            raise ValueError(\n",
    "                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n",
    "                    MODEL_CLASSES.keys()))\n",
    "        if self.model_name_or_path is None:\n",
    "            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file_dir: Optional[str] = field(default=\"../data/finetune\", metadata={\"help\": \"The train jsonl data file folder.\"})\n",
    "    validation_file_dir: Optional[str] = field(default=\"../data/finetune\", metadata={\"help\": \"The evaluation jsonl file folder.\"})\n",
    "    max_source_length: Optional[int] = field(default=256, metadata={\"help\": \"Max length of prompt input text\"})\n",
    "    max_target_length: Optional[int] = field(default=256, metadata={\"help\": \"Max length of output text\"})\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=1000,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=10,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[float] = field(\n",
    "        default=0.05,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PeftArguments(TrainingArguments):\n",
    "    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n",
    "    target_modules: Optional[str] = field(default=\"all\")\n",
    "    lora_rank: Optional[int] = field(default=8)\n",
    "    lora_dropout: Optional[float] = field(default=0.05)\n",
    "    lora_alpha: Optional[float] = field(default=32.0)\n",
    "    modules_to_save: Optional[str] = field(default=None)\n",
    "    peft_path: Optional[str] = field(default=None)\n",
    "    \n",
    "    # notebook 直接写进参数\n",
    "    output_dir: str = field(default='outputs-sft')\n",
    "    do_train: bool = field(default=True)\n",
    "    do_eval: bool = field(default=True)\n",
    "    fp16: bool = field(default=True)\n",
    "    num_train_epochs: float = field(default=0.5)\n",
    "    learning_rate: float = field(default=2e-5)\n",
    "    warmup_steps: int = field(default=10)\n",
    "    weight_decay: float = field(default=0.01)\n",
    "    logging_strategy: str = field(default='steps')\n",
    "    logging_steps: int = field(default=10)\n",
    "    evaluation_strategy: str = field(default='steps')\n",
    "    eval_steps: int = field(default=10)\n",
    "    save_strategy: str = field(default='steps')\n",
    "    save_steps: int = field(default=500)\n",
    "    save_total_limit: int = field(default=3)\n",
    "    seed: int = field(default=42)\n",
    "    per_device_train_batch_size: int = field(default=4)\n",
    "    per_device_eval_batch_size: int = field(default=4)\n",
    "    overwrite_output_dir: bool = field(default=True)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    report_to: str = field(default=\"tensorboard\")\n",
    "    gradient_checkpointing: bool = field(default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftArguments(output_dir='outputs-sft', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.5, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=10, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='outputs-sft/runs/Jun07_17-43-30_ts-be7e7b4bf1f24d21bb9c3d60ec0c1ae2-launcher', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=3, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='outputs-sft', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=32.0, modules_to_save=None, peft_path=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = ModelArguments()\n",
    "data_args = DataTrainingArguments()\n",
    "training_args = PeftArguments()\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:30.762 | WARNING  | __main__:<cell line: 3>:3 - Model args: ModelArguments(model_type='bloom', model_name_or_path='../../../models/bigscience/bloomz-560m', tokenizer_name_or_path=None, load_in_8bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True)\n",
      "2023-06-07 17:43:30.763 | WARNING  | __main__:<cell line: 4>:4 - Data args: DataTrainingArguments(dataset_name=None, dataset_config_name=None, train_file_dir='../data/finetune', validation_file_dir='../data/finetune', max_source_length=256, max_target_length=256, max_train_samples=1000, max_eval_samples=10, overwrite_cache=False, validation_split_percentage=0.05, preprocessing_num_workers=None)\n",
      "2023-06-07 17:43:30.765 | WARNING  | __main__:<cell line: 5>:5 - Training args: PeftArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-sft/runs/Jun07_17-43-30_ts-be7e7b4bf1f24d21bb9c3d60ec0c1ae2-launcher,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lora_alpha=32.0,\n",
      "lora_dropout=0.05,\n",
      "lora_rank=8,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "modules_to_save=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=0.5,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=outputs-sft,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "peft_path=None,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-sft,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "target_modules=all,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_peft=True,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "2023-06-07 17:43:30.766 | WARNING  | __main__:<cell line: 6>:6 - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True\n"
     ]
    }
   ],
   "source": [
    "send_example_telemetry(\"run_sft\", model_args, data_args)\n",
    "\n",
    "logger.warning(f\"Model args: {model_args}\")\n",
    "logger.warning(f\"Data args: {data_args}\")\n",
    "logger.warning(f\"Training args: {training_args}\")\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义各函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastOutputToFloat(torch.nn.Sequential):\n",
    "    \"\"\"Cast the output of the model to float\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "class SavePeftModelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer for lora models\n",
    "    \"\"\"\n",
    "\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        \"\"\"Save the LoRA model.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "def save_model(output_dir, model, tokenizer, args):\n",
    "    \"\"\"Save the model and the tokenizer.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Take care of distributed/parallel training\n",
    "    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    torch.save(args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def find_all_linear_names(peft_model, int4=False, int8=False):\n",
    "    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    if int4 or int8:\n",
    "        import bitsandbytes as bnb\n",
    "        if int4:\n",
    "            cls = bnb.nn.Linear4bit\n",
    "        elif int8:\n",
    "            cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            # last layer is not add to lora_module_names\n",
    "            if 'lm_head' in name:\n",
    "                continue\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return sorted(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1024)\n",
       "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): CastOutputToFloat(\n",
       "    (0): Linear(in_features=1024, out_features=250880, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "if not model_args.model_type:\n",
    "    raise ValueError(\"Please specify a model_type, e.g. llama, chatglm, bloom, etc.\")\n",
    "model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    model = model_class.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        load_in_8bit=model_args.load_in_8bit,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=model_args.device_map,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "    )\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "else:\n",
    "    raise ValueError(f\"Error, model_name_or_path is None, SFT must be loaded from a pre-trained model\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='../../../models/bigscience/bloomz-560m', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "tokenizer_name_or_path = model_args.tokenizer_name_or_path\n",
    "if not tokenizer_name_or_path:\n",
    "    tokenizer_name_or_path = model_args.model_name_or_path\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n",
    "# Required for llama\n",
    "if model_args.model_type == \"llama\" and tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": DEFAULT_PAD_TOKEN})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:41.896 | INFO     | __main__:<cell line: 1>:6 - Init new peft model\n",
      "2023-06-07 17:43:41.897 | INFO     | __main__:<cell line: 1>:13 - Peft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\n",
      "2023-06-07 17:43:41.898 | INFO     | __main__:<cell line: 1>:14 - Peft lora_rank: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3145728 || all params: 562360320 || trainable%: 0.5593794384354857\n"
     ]
    }
   ],
   "source": [
    "if training_args.use_peft:\n",
    "    if training_args.peft_path is not None:\n",
    "        logger.info(f\"Peft from pre-trained model: {training_args.peft_path}\")\n",
    "        model = PeftModel.from_pretrained(model, training_args.peft_path, is_trainable=True)\n",
    "    else:\n",
    "        logger.info(\"Init new peft model\")\n",
    "        target_modules = training_args.target_modules.split(',') if training_args.target_modules else None\n",
    "        if target_modules and 'all' in target_modules:\n",
    "            target_modules = find_all_linear_names(model, int4=False, int8=model_args.load_in_8bit)\n",
    "        modules_to_save = training_args.modules_to_save\n",
    "        if modules_to_save is not None:\n",
    "            modules_to_save = modules_to_save.split(',')\n",
    "        logger.info(f\"Peft target_modules: {target_modules}\")\n",
    "        logger.info(f\"Peft lora_rank: {training_args.lora_rank}\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=target_modules,\n",
    "            inference_mode=False,\n",
    "            r=training_args.lora_rank,\n",
    "            lora_alpha=training_args.lora_alpha,\n",
    "            lora_dropout=training_args.lora_dropout,\n",
    "            modules_to_save=modules_to_save)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    if model_args.load_in_8bit:\n",
    "        model = prepare_model_for_int8_training(model)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    logger.info(\"Full parameters training\")\n",
    "    print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:45.653 | INFO     | __main__:<cell line: 2>:27 - train files: ../data/finetune/Belle_open_source_1k.json\n",
      "2023-06-07 17:43:45.655 | INFO     | __main__:<cell line: 2>:32 - eval files: ../data/finetune/Belle_open_source_1k.json\n",
      "Found cached dataset json (/home/flemingxu/.cache/huggingface/datasets/json/default-81868ba195386786/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76760a5e6e6a4f28acb0d05c22c507e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:46.858 | INFO     | __main__:<cell line: 53>:53 - Raw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Get datasets\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n",
    "        train_data_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n",
    "            f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n",
    "        logger.info(f\"train files: {', '.join(train_data_files)}\")\n",
    "        data_files[\"train\"] = train_data_files\n",
    "    if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n",
    "        eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n",
    "            f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n",
    "        logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n",
    "        data_files[\"validation\"] = eval_data_files\n",
    "    raw_datasets = load_dataset(\n",
    "        'json',\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            'json',\n",
    "            data_files=data_files,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            'json',\n",
    "            data_files=data_files,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "logger.info(f\"Raw datasets: {raw_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:46.904 | DEBUG    | __main__:<cell line: 33>:41 - Example train_dataset[0]: {'instruction': '为给定的句子生成一个同义句。\\nShe is studying for her final exams.', 'input': '', 'output': 'She is preparing for her last exams.'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:48.001 | DEBUG    | __main__:<cell line: 33>:51 - Num train_samples: 1000\n",
      "2023-06-07 17:43:48.002 | DEBUG    | __main__:<cell line: 33>:52 - Tokenized training example:\n",
      "2023-06-07 17:43:48.003 | DEBUG    | __main__:<cell line: 33>:53 - Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "根据给定的段落，对其进行编辑，使其更清楚或更简洁。\n",
      "这段话来自一篇科学文章，但表达得不够清楚：\n",
      "“本次实验的过程包括了获得实验组的样本、随机分配样本、实施处理并记录结果。最后，使用统计学方法分析数据以得出结论。”\n",
      "\n",
      "\n",
      "### Response: 本实验包括：获取实验样本、随机分配处理、记录实验结果并使用统计学方法分析数据得出结论。</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets\n",
    "max_source_length = data_args.max_source_length\n",
    "max_target_length = data_args.max_target_length\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    sources = []\n",
    "    targets = []\n",
    "    for instruction, input, output in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        if input:\n",
    "            instruction = instruction + '\\n' + input\n",
    "        source = PROMPT_TEMPLATE.format_map({'instruction': instruction})\n",
    "        target = f\"{output}{tokenizer.eos_token}\"\n",
    "\n",
    "        sources.append(source)\n",
    "        targets.append(target)\n",
    "\n",
    "    tokenized_sources = tokenizer(sources, truncation=True, max_length=max_source_length)\n",
    "    tokenized_targets = tokenizer(targets, add_special_tokens=False, truncation=True, max_length=max_target_length)\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_labels = []\n",
    "    for s, t in zip(tokenized_sources['input_ids'], tokenized_targets['input_ids']):\n",
    "        input_ids = torch.LongTensor(s + t)\n",
    "        labels = torch.LongTensor([IGNORE_INDEX] * (max_source_length + max_target_length - len(t)) + t)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_labels.append(labels)\n",
    "    results = {'input_ids': all_input_ids, 'labels': all_labels}\n",
    "\n",
    "    return results\n",
    "\n",
    "train_dataset = None\n",
    "max_train_samples = 0\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets['train']\n",
    "    max_train_samples = len(train_dataset)\n",
    "    if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n",
    "    with training_args.main_process_first(desc=\"Train dataset tokenization\"):\n",
    "        train_dataset = train_dataset.shuffle().map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n",
    "        logger.debug(\"Tokenized training example:\")\n",
    "        logger.debug(tokenizer.decode(train_dataset[0]['input_ids']))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:48.047 | DEBUG    | __main__:<cell line: 3>:12 - Example eval_dataset[0]: {'instruction': '为给定的句子生成一个同义句。\\nShe is studying for her final exams.', 'input': '', 'output': 'She is preparing for her last exams.'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:43:48.635 | DEBUG    | __main__:<cell line: 3>:21 - Num eval_samples: 10\n",
      "2023-06-07 17:43:48.636 | DEBUG    | __main__:<cell line: 3>:22 - Tokenized eval example:\n",
      "2023-06-07 17:43:48.638 | DEBUG    | __main__:<cell line: 3>:23 - Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "为给定的句子生成一个同义句。\n",
      "She is studying for her final exams.\n",
      "\n",
      "### Response: She is preparing for her last exams.</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = None\n",
    "max_eval_samples = 0\n",
    "if training_args.do_eval:\n",
    "    with training_args.main_process_first(desc=\"Eval dataset tokenization\"):\n",
    "        if \"validation\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = raw_datasets[\"validation\"]\n",
    "        max_eval_samples = len(eval_dataset)\n",
    "        if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "        logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=eval_dataset.column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n",
    "        logger.debug(\"Tokenized eval example:\")\n",
    "        logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))\n",
    "        \n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.config.use_cache = False\n",
    "else:\n",
    "    model.config.use_cache = True\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=max_source_length + max_target_length\n",
    ")\n",
    "trainer = SavePeftModelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:45:21.604 | INFO     | __main__:<cell line: 2>:3 - *** Train ***\n",
      "2023-06-07 17:45:22.130 | DEBUG    | __main__:<cell line: 2>:4 - Train dataloader example: {'input_ids': tensor([[     3,      3,      3,  ..., 218591,    420,      2],\n",
      "        [     3,      3,      3,  ...,  75844,    420,      2],\n",
      "        [     3,      3,      3,  ...,   8401,    420,      2],\n",
      "        [     3,      3,      3,  ..., 215090,    420,      2]]), 'labels': tensor([[  -100,   -100,   -100,  ..., 218591,    420,      2],\n",
      "        [  -100,   -100,   -100,  ...,  75844,    420,      2],\n",
      "        [  -100,   -100,   -100,  ...,   8401,    420,      2],\n",
      "        [  -100,   -100,   -100,  ..., 215090,    420,      2]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])}\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.903400</td>\n",
       "      <td>3.215630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.830500</td>\n",
       "      <td>3.142511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.678200</td>\n",
       "      <td>3.100051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.816300</td>\n",
       "      <td>3.042619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.393300</td>\n",
       "      <td>3.007197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.552900</td>\n",
       "      <td>2.991486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.609200</td>\n",
       "      <td>2.973372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.809700</td>\n",
       "      <td>2.954670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.475300</td>\n",
       "      <td>2.945711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.860300</td>\n",
       "      <td>2.944793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.649100</td>\n",
       "      <td>2.937002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.119600</td>\n",
       "      <td>2.934451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:46:26.448 | DEBUG    | __main__:<cell line: 2>:12 - Training metrics: {'train_runtime': 64.2933, 'train_samples_per_second': 7.777, 'train_steps_per_second': 1.944, 'total_flos': 469185331200000.0, 'train_loss': 2.719934181213379, 'epoch': 0.5, 'train_samples': 1000}\n",
      "2023-06-07 17:46:26.471 | INFO     | __main__:<cell line: 2>:16 - Saving model checkpoint to outputs-sft\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        0.5\n",
      "  total_flos               =   436962GF\n",
      "  train_loss               =     2.7199\n",
      "  train_runtime            = 0:01:04.29\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      7.777\n",
      "  train_steps_per_second   =      1.944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 64.2933,\n",
       " 'train_samples_per_second': 7.777,\n",
       " 'train_steps_per_second': 1.944,\n",
       " 'total_flos': 469185331200000.0,\n",
       " 'train_loss': 2.719934181213379,\n",
       " 'epoch': 0.5,\n",
       " 'train_samples': 1000}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "if training_args.do_train:\n",
    "    logger.info(\"*** Train ***\")\n",
    "    logger.debug(f\"Train dataloader example: {list(trainer.get_train_dataloader())[0]}\")\n",
    "    checkpoint = None\n",
    "\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    metrics[\"train_samples\"] = max_train_samples\n",
    "    logger.debug(f\"Training metrics: {metrics}\")\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n",
    "    save_model(training_args.output_dir, model, tokenizer, training_args)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:46:26.778 | INFO     | __main__:<cell line: 2>:3 - *** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:46:27.118 | DEBUG    | __main__:<cell line: 2>:12 - Eval metrics: {'eval_loss': 2.934418201446533, 'eval_runtime': 0.3326, 'eval_samples_per_second': 30.068, 'eval_steps_per_second': 9.021, 'epoch': 0.5, 'eval_samples': 10, 'perplexity': 18.81055599670998}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        0.5\n",
      "  eval_loss               =     2.9344\n",
      "  eval_runtime            = 0:00:00.33\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     30.068\n",
      "  eval_steps_per_second   =      9.021\n",
      "  perplexity              =    18.8106\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    metrics[\"eval_samples\"] = max_eval_samples\n",
    "    try:\n",
    "        perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    metrics[\"perplexity\"] = perplexity\n",
    "    logger.debug(f\"Eval metrics: {metrics}\")\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs-sft'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "adapter_config.json  \u001B[0m\u001B[01;34mruns\u001B[0m/                    train_results.json\n",
      "adapter_model.bin    special_tokens_map.json  trainer_state.json\n",
      "all_results.json     tokenizer.json           training_args.bin\n",
      "eval_results.json    tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls outputs-sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`\n",
    "- 日志保存在`output_dir/logs`\n",
    "\n",
    "查看运行日志，默认使用的是tensorboard保存日志，启动方式如下：\n",
    "\n",
    "\n",
    "```\n",
    "tensorboard --logdir output_dir/logs --host 0.0.0.0 --port 8009\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38xm",
   "language": "python",
   "name": "py38kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
