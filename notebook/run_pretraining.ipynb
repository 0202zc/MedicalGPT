{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "| Stage 1: Continue Pretraining   | 增量预训练，在海量领域文本数据上继续预训练GPT模型，以注入领域知识       | [scripts/pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/scripts/pretraining.py) | [scripts/run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/scripts/run_pt.sh)    | [run_pretraining.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/notebook/run_pretraining.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_pretraining.ipynb)           |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 1: Continue Pretraining\n",
    "\n",
    "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练LLaMA类GPT模型，以注入领域知识\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置运行环境\n",
    "\n",
    "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "# %cd MedicalGPT\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装库和依赖包：\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers>=4.28.1\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm>=4.47.0\n",
    "peft>=0.3.0\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 咱们开始吧\n",
    "\n",
    "环境配置完成，开始导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:12:14.498548: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 17:12:14.659578: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-07 17:12:16.415986: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 17:12:16.416129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 17:12:16.416138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('disk/nlp/MedicalGPT/notebook/run_pretraining.ipynb')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright 2023 XuMing(xuming624@qq.com) and The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n",
    "\n",
    "part of this code is adapted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py\n",
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "from typing import Optional, List, Dict, Any, Mapping\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_int8_training\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    BloomForCausalLM,\n",
    "    AutoModel,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    BloomTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer import TRAINING_ARGS_NAME\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n",
    "    \"chatglm\": (AutoModel, AutoTokenizer),\n",
    "    \"llama\": (LlamaForCausalLM, LlamaTokenizer),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_article_tail500.txt  tianlongbabu.txt\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/pretrain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_type: str = field(\n",
    "        default='bloom',\n",
    "        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n",
    "    )\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"bigscience/bloomz-560m\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=\"float16\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    device_map: Optional[str] = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_type is None:\n",
    "            raise ValueError(\n",
    "                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n",
    "                    MODEL_CLASSES.keys()))\n",
    "        if self.model_name_or_path is None:\n",
    "            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file_dir: Optional[str] = field(default=\"../data/pretrain/\", metadata={\"help\": \"The train text data file folder.\"})\n",
    "    validation_file_dir: Optional[str] = field(\n",
    "        default=\"../data/pretrain/\",\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on text file folder.\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=100000,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=10,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n",
    "    block_size: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Optional input sequence length after tokenization. \"\n",
    "                \"The training dataset will be truncated in block of this size for training. \"\n",
    "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[float] = field(\n",
    "        default=0.05,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    keep_linebreaks: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.streaming:\n",
    "            require_version(\"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PeftArguments(TrainingArguments):\n",
    "    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n",
    "    target_modules: Optional[str] = field(default=\"all\")\n",
    "    lora_rank: Optional[int] = field(default=8)\n",
    "    lora_dropout: Optional[float] = field(default=0.05)\n",
    "    lora_alpha: Optional[float] = field(default=32.0)\n",
    "    modules_to_save: Optional[str] = field(default=None)\n",
    "    peft_path: Optional[str] = field(default=None)\n",
    "    \n",
    "    # notebook 直接写进参数\n",
    "    output_dir: str = field(default='outputs-pt')\n",
    "    do_train: bool = field(default=True)\n",
    "    do_eval: bool = field(default=True)\n",
    "    fp16: bool = field(default=True)\n",
    "    num_train_epochs: float = field(default=0.5)\n",
    "    learning_rate: float = field(default=2e-5)\n",
    "    warmup_steps: int = field(default=10)\n",
    "    weight_decay: float = field(default=0.01)\n",
    "    logging_strategy: str = field(default='steps')\n",
    "    logging_steps: int = field(default=10)\n",
    "    evaluation_strategy: str = field(default='steps')\n",
    "    eval_steps: int = field(default=10)\n",
    "    save_strategy: str = field(default='steps')\n",
    "    save_steps: int = field(default=500)\n",
    "    save_total_limit: int = field(default=3)\n",
    "    seed: int = field(default=42)\n",
    "    per_device_train_batch_size: int = field(default=4)\n",
    "    per_device_eval_batch_size: int = field(default=4)\n",
    "    overwrite_output_dir: bool = field(default=True)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    report_to: str = field(default=\"tensorboard\")\n",
    "    gradient_checkpointing: bool = field(default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftArguments(output_dir='outputs-pt', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=0.5, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=10, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='outputs-pt/runs/Jun07_17-12-57_ts-be7e7b4bf1f24d21bb9c3d60ec0c1ae2-launcher', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=3, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name='outputs-pt', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_HF: 'adamw_hf'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=32.0, modules_to_save=None, peft_path=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = ModelArguments()\n",
    "data_args = DataTrainingArguments()\n",
    "training_args = PeftArguments()\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:12:58.270 | WARNING  | __main__:<cell line: 5>:5 - Model args: ModelArguments(model_type='bloom', model_name_or_path='../../../models/bigscience/bloomz-560m', tokenizer_name_or_path=None, load_in_8bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True)\n",
      "2023-06-07 17:12:58.271 | WARNING  | __main__:<cell line: 6>:6 - Data args: DataTrainingArguments(dataset_name=None, dataset_config_name=None, train_file_dir='../data/pretrain/', validation_file_dir='../data/pretrain/', max_train_samples=100000, max_eval_samples=10, streaming=False, block_size=1024, overwrite_cache=False, validation_split_percentage=0.05, preprocessing_num_workers=None, keep_linebreaks=True)\n",
      "2023-06-07 17:12:58.272 | WARNING  | __main__:<cell line: 7>:7 - Training args: PeftArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-pt/runs/Jun07_17-12-57_ts-be7e7b4bf1f24d21bb9c3d60ec0c1ae2-launcher,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lora_alpha=32.0,\n",
      "lora_dropout=0.05,\n",
      "lora_rank=8,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "modules_to_save=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=0.5,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=outputs-pt,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "peft_path=None,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-pt,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "target_modules=all,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_peft=True,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "2023-06-07 17:12:58.272 | WARNING  | __main__:<cell line: 8>:8 - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True\n"
     ]
    }
   ],
   "source": [
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "send_example_telemetry(\"run_pt\", model_args, data_args)\n",
    "\n",
    "logger.warning(f\"Model args: {model_args}\")\n",
    "logger.warning(f\"Data args: {data_args}\")\n",
    "logger.warning(f\"Training args: {training_args}\")\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义各函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, references, normalize=True, sample_weight=None):\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight))\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics, we need to shift the labels\n",
    "    labels = labels[:, 1:].reshape(-1)\n",
    "    preds = preds[:, :-1].reshape(-1)\n",
    "    return accuracy(predictions=preds, references=labels)\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n",
    "    if not isinstance(features[0], Mapping):\n",
    "        features = [vars(f) for f in features]\n",
    "    first = features[0]\n",
    "    batch = {}\n",
    "\n",
    "    # Special handling for labels.\n",
    "    # Ensure that tensor is created with the correct type\n",
    "    if \"label\" in first and first[\"label\"] is not None:\n",
    "        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n",
    "        dtype = torch.long if isinstance(label, int) else torch.float\n",
    "        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n",
    "    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n",
    "        if isinstance(first[\"label_ids\"], torch.Tensor):\n",
    "            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n",
    "        else:\n",
    "            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n",
    "            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n",
    "\n",
    "    # Handling of all other possible keys.\n",
    "    # Again, we will use the first element to figure out which key/values are not None for this model.\n",
    "    try:\n",
    "        for k, v in first.items():\n",
    "            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "                elif isinstance(v, np.ndarray):\n",
    "                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n",
    "                else:\n",
    "                    batch[k] = torch.tensor([f[k] for f in features])\n",
    "    except ValueError:  # quick fix by simply take the first example\n",
    "        for k, v in first.items():\n",
    "            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    batch[k] = torch.stack([features[0][k]] * len(features))\n",
    "                elif isinstance(v, np.ndarray):\n",
    "                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n",
    "                else:\n",
    "                    batch[k] = torch.tensor([features[0][k]] * len(features))\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "class GroupTextsBuilder:\n",
    "    def __init__(self, max_seq_length):\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Concatenate all texts.\n",
    "        firsts = {k: examples[k][0][0] for k in examples.keys()}\n",
    "        lasts = {k: examples[k][0][-1] for k in examples.keys()}\n",
    "        contents = {k: sum([vi[1:-1] for vi in v], []) for k, v in examples.items()}\n",
    "        total_length = len(contents[list(examples.keys())[0]])\n",
    "\n",
    "        content_length = self.max_seq_length - 2\n",
    "        if total_length >= content_length:\n",
    "            total_length = (total_length // content_length) * content_length\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [[firsts[k]] + t[i: i + content_length] + [lasts[k]] for i in range(0, total_length, content_length)] for\n",
    "            k, t in contents.items()}\n",
    "        return result\n",
    "\n",
    "\n",
    "class SavePeftModelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer for lora models\n",
    "    \"\"\"\n",
    "\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        \"\"\"Save the LoRA model.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "def save_model(output_dir, model, tokenizer, args):\n",
    "    \"\"\"Save the model and the tokenizer.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Take care of distributed/parallel training\n",
    "    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    torch.save(args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def find_all_linear_names(peft_model, int4=False, int8=False):\n",
    "    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    if int4 or int8:\n",
    "        import bitsandbytes as bnb\n",
    "        if int4:\n",
    "            cls = bnb.nn.Linear4bit\n",
    "        elif int8:\n",
    "            cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            # last layer is not add to lora_module_names\n",
    "            if 'lm_head' in name:\n",
    "                continue\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return sorted(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1024)\n",
       "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "if not model_args.model_type:\n",
    "    raise ValueError(\"Please specify a model_type, e.g. llama, chatglm, bloom, etc.\")\n",
    "model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n",
    "if model_args.model_type and model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    model = model_class.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        load_in_8bit=model_args.load_in_8bit,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=model_args.device_map,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Error, model_name_or_path is None, Continue PT must be loaded from a pre-trained model\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:13:09.343 | INFO     | __main__:<cell line: 11>:16 - Init new peft model\n",
      "2023-06-07 17:13:09.344 | INFO     | __main__:<cell line: 11>:23 - Peft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\n",
      "2023-06-07 17:13:09.345 | INFO     | __main__:<cell line: 11>:24 - Peft lora_rank: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3145728 || all params: 562360320 || trainable%: 0.5593794384354857\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "tokenizer_name_or_path = model_args.tokenizer_name_or_path\n",
    "if not tokenizer_name_or_path:\n",
    "    tokenizer_name_or_path = model_args.model_name_or_path\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n",
    "\n",
    "if training_args.use_peft:\n",
    "    if training_args.peft_path is not None:\n",
    "        logger.info(f\"Peft from pre-trained model: {training_args.peft_path}\")\n",
    "        model = PeftModel.from_pretrained(model, training_args.peft_path, is_trainable=True)\n",
    "    else:\n",
    "        logger.info(\"Init new peft model\")\n",
    "        target_modules = training_args.target_modules.split(',') if training_args.target_modules else None\n",
    "        if target_modules and 'all' in target_modules:\n",
    "            target_modules = find_all_linear_names(model, int4=False, int8=model_args.load_in_8bit)\n",
    "        modules_to_save = training_args.modules_to_save\n",
    "        if modules_to_save is not None:\n",
    "            modules_to_save = modules_to_save.split(',')\n",
    "        logger.info(f\"Peft target_modules: {target_modules}\")\n",
    "        logger.info(f\"Peft lora_rank: {training_args.lora_rank}\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=target_modules,\n",
    "            inference_mode=False,\n",
    "            r=training_args.lora_rank,\n",
    "            lora_alpha=training_args.lora_alpha,\n",
    "            lora_dropout=training_args.lora_dropout,\n",
    "            modules_to_save=modules_to_save)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    if model_args.load_in_8bit:\n",
    "        model = prepare_model_for_int8_training(model)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    logger.info(\"Full parameters training\")\n",
    "    print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "if data_args.block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > 1024:\n",
    "        logger.warning(\n",
    "            \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
    "            \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
    "            \" override this default with `--block_size xxx`.\"\n",
    "        )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if data_args.block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(data_args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:20:00.174 | INFO     | __main__:<cell line: 10>:38 - train files: ../data/pretrain/en_article_tail500.txt, ../data/pretrain/tianlongbabu.txt\n",
      "2023-06-07 17:20:00.175 | INFO     | __main__:<cell line: 10>:42 - eval files: ../data/pretrain/en_article_tail500.txt, ../data/pretrain/tianlongbabu.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/flemingxu/.cache/huggingface/datasets/text/default-73457fee1e04c74f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de975f887e9347e182f6603c361838ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdf96d9eb074f5db9b7fb9e39a04c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/flemingxu/.cache/huggingface/datasets/text/default-73457fee1e04c74f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feabd472c8eb4ddcbf0b24e3dac0b944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:20:03.634 | INFO     | __main__:<cell line: 68>:68 - Raw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2880\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2880\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "# 'text' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        streaming=data_args.streaming,\n",
    "    )\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            streaming=data_args.streaming,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            streaming=data_args.streaming,\n",
    "        )\n",
    "else:\n",
    "    data_files = {}\n",
    "    dataset_args = {}\n",
    "    if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n",
    "        train_data_files = glob(f'{data_args.train_file_dir}/**/*.txt', recursive=True)\n",
    "        logger.info(f\"train files: {', '.join(train_data_files)}\")\n",
    "        data_files[\"train\"] = train_data_files\n",
    "    if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n",
    "        eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.txt', recursive=True)\n",
    "        logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n",
    "        data_files[\"validation\"] = eval_data_files\n",
    "    extension = \"text\"\n",
    "    dataset_args[\"keep_linebreaks\"] = data_args.keep_linebreaks\n",
    "    raw_datasets = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        **dataset_args,\n",
    "    )\n",
    "    # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            extension,\n",
    "            data_files=data_files,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            **dataset_args,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            extension,\n",
    "            data_files=data_files,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            **dataset_args,\n",
    "        )\n",
    "logger.info(f\"Raw datasets: {raw_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/2880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1024:   0%|          | 0/2880 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 226\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 226\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets.\n",
    "if training_args.do_train:\n",
    "    column_names = list(raw_datasets[\"train\"].features)\n",
    "else:\n",
    "    column_names = list(raw_datasets[\"validation\"].features)\n",
    "\n",
    "with training_args.main_process_first(desc=\"Dataset tokenization and grouping\"):\n",
    "    if not data_args.streaming:\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "        lm_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )\n",
    "    else:\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "        lm_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "        )\n",
    "\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:20:05.650 | DEBUG    | __main__:<cell line: 3>:11 - Num train_samples: 226\n",
      "2023-06-07 17:20:05.651 | DEBUG    | __main__:<cell line: 3>:12 - Tokenized training example:\n",
      "2023-06-07 17:20:05.655 | DEBUG    | __main__:<cell line: 3>:13 - contract to work in specified mines and mills. There seemed to be no\n",
      "limit to the factories, forges, refineries, and railways that could be\n",
      "built, to the multitudes that could be employed in conquering a\n",
      "continent. As for the future, that was in the hands of Providence!\n",
      "\n",
      "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
      "and the planters of Calhoun's had their theories of government and\n",
      "politics, so the leaders in business enterprise had theirs. It was\n",
      "simple and easily stated. \"It is the duty of the government,\" they\n",
      "urged, \"to protect American industry against foreign competition by\n",
      "means of high tariffs on imported goods, to aid railways by generous\n",
      "grants of land, to sell mineral and timber lands at low prices to\n",
      "energetic men ready to develop them, and then to leave the rest to the\n",
      "initiative and drive of individuals and companies.\" All government\n",
      "interference with the management, prices, rates, charges, and conduct of\n",
      "private business they held to be either wholly pernicious or intolerably\n",
      "impertinent. Judging from their speeches and writings, they conceived\n",
      "the nation as a great collection of individuals, companies, and labor\n",
      "unions all struggling for profits or high wages and held together by a\n",
      "government whose principal duty was to keep the peace among them and\n",
      "protect industry against the foreign manufacturer. Such was the\n",
      "political theory of business during the generation that followed the\n",
      "Civil War.\n",
      "\n",
      "\n",
      "THE SUPREMACY OF THE REPUBLICAN PARTY (1861-85)\n",
      "\n",
      "=Business Men and Republican Policies.=--Most of the leaders in industry\n",
      "gravitated to the Republican ranks. They worked in the North and the\n",
      "Republican party was essentially Northern. It was moreover--at least so\n",
      "far as the majority of its members were concerned--committed to\n",
      "protective tariffs, a sound monetary and banking system, the promotion\n",
      "of railways and industry by land grants, and the development of internal\n",
      "improvements. It was furthermore generous in its immigration policy. It\n",
      "proclaimed America to be an asylum for the oppressed of all countries\n",
      "and flung wide the doors for immigrants eager to fill the factories, man\n",
      "the mines, and settle upon Western lands. In a word the Republicans\n",
      "stood for all those specific measures which favored the enlargement and\n",
      "prosperity of business. At the same time they resisted government\n",
      "interference with private enterprise. They did not regulate railway\n",
      "rates, prosecute trusts for forming combinations, or prevent railway\n",
      "companies from giving lower rates to some shippers than to others. To\n",
      "sum it up, the political theories of the Republican party for three\n",
      "decades after the Civil War were the theories of American\n",
      "business--prosperous and profitable industries for the owners and \"the\n",
      "full dinner pail\" for the workmen. Naturally a large portion of those\n",
      "who flourished under its policies gave their support to it, voted for\n",
      "its candidates, and subscribed to its campaign funds.\n",
      "\n",
      "=Sources of Republican Strength in the North.=--The Republican party was\n",
      "in fact a political organization of singular power. It originated in a\n",
      "wave of moral enthusiasm, having attracted to itself, if not the\n",
      "abolitionists, certainly all those idealists, like James Russell Lowell\n",
      "and George William Curtis, who had opposed slavery when opposition was\n",
      "neither safe nor popular. To moral principles it added practical\n",
      "considerations. Business men had confidence in it. Workingmen, who\n",
      "longed for the independence of the farmer, owed to its indulgent land\n",
      "policy the opportunity of securing free homesteads in the West. The\n",
      "immigrant, landing penniless on these shores, as a result of the same\n",
      "beneficent system, often found himself in a little while with an estate\n",
      "as large as many a baronial domain in the Old World. Under a Republican\n",
      "administration, the union had been saved. To it the veterans of the war\n",
      "could turn with confidence for those rewards of service which the\n",
      "government could bestow: pensions surpassing in liberality anything that\n",
      "the world had ever seen. Under a Republican administration also the\n",
      "great debt had been created in the defense of the union, and to the\n",
      "Republican party every investor in government bonds could look for the\n",
      "full and honorable discharge of the interest and principal. The spoils\n",
      "system, inaugurated by Jacksonian Democracy, in turn placed all the\n",
      "federal offices in Republican hands, furnishing an army of party workers\n",
      "to be counted on for loyal service in every campaign.\n",
      "\n",
      "Of all these things Republican leaders made full and vigorous use,\n",
      "sometimes ascribing to the party, in accordance with ancient political\n",
      "usage, merits and achievements not wholly its own. Particularly was this\n",
      "true in the case of saving the union\n",
      "2023-06-07 17:20:05.657 | DEBUG    | __main__:<cell line: 17>:25 - Num eval_samples: 10\n",
      "2023-06-07 17:20:05.658 | DEBUG    | __main__:<cell line: 17>:26 - Tokenized eval example:\n",
      "2023-06-07 17:20:05.662 | DEBUG    | __main__:<cell line: 17>:27 - contract to work in specified mines and mills. There seemed to be no\n",
      "limit to the factories, forges, refineries, and railways that could be\n",
      "built, to the multitudes that could be employed in conquering a\n",
      "continent. As for the future, that was in the hands of Providence!\n",
      "\n",
      "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
      "and the planters of Calhoun's had their theories of government and\n",
      "politics, so the leaders in business enterprise had theirs. It was\n",
      "simple and easily stated. \"It is the duty of the government,\" they\n",
      "urged, \"to protect American industry against foreign competition by\n",
      "means of high tariffs on imported goods, to aid railways by generous\n",
      "grants of land, to sell mineral and timber lands at low prices to\n",
      "energetic men ready to develop them, and then to leave the rest to the\n",
      "initiative and drive of individuals and companies.\" All government\n",
      "interference with the management, prices, rates, charges, and conduct of\n",
      "private business they held to be either wholly pernicious or intolerably\n",
      "impertinent. Judging from their speeches and writings, they conceived\n",
      "the nation as a great collection of individuals, companies, and labor\n",
      "unions all struggling for profits or high wages and held together by a\n",
      "government whose principal duty was to keep the peace among them and\n",
      "protect industry against the foreign manufacturer. Such was the\n",
      "political theory of business during the generation that followed the\n",
      "Civil War.\n",
      "\n",
      "\n",
      "THE SUPREMACY OF THE REPUBLICAN PARTY (1861-85)\n",
      "\n",
      "=Business Men and Republican Policies.=--Most of the leaders in industry\n",
      "gravitated to the Republican ranks. They worked in the North and the\n",
      "Republican party was essentially Northern. It was moreover--at least so\n",
      "far as the majority of its members were concerned--committed to\n",
      "protective tariffs, a sound monetary and banking system, the promotion\n",
      "of railways and industry by land grants, and the development of internal\n",
      "improvements. It was furthermore generous in its immigration policy. It\n",
      "proclaimed America to be an asylum for the oppressed of all countries\n",
      "and flung wide the doors for immigrants eager to fill the factories, man\n",
      "the mines, and settle upon Western lands. In a word the Republicans\n",
      "stood for all those specific measures which favored the enlargement and\n",
      "prosperity of business. At the same time they resisted government\n",
      "interference with private enterprise. They did not regulate railway\n",
      "rates, prosecute trusts for forming combinations, or prevent railway\n",
      "companies from giving lower rates to some shippers than to others. To\n",
      "sum it up, the political theories of the Republican party for three\n",
      "decades after the Civil War were the theories of American\n",
      "business--prosperous and profitable industries for the owners and \"the\n",
      "full dinner pail\" for the workmen. Naturally a large portion of those\n",
      "who flourished under its policies gave their support to it, voted for\n",
      "its candidates, and subscribed to its campaign funds.\n",
      "\n",
      "=Sources of Republican Strength in the North.=--The Republican party was\n",
      "in fact a political organization of singular power. It originated in a\n",
      "wave of moral enthusiasm, having attracted to itself, if not the\n",
      "abolitionists, certainly all those idealists, like James Russell Lowell\n",
      "and George William Curtis, who had opposed slavery when opposition was\n",
      "neither safe nor popular. To moral principles it added practical\n",
      "considerations. Business men had confidence in it. Workingmen, who\n",
      "longed for the independence of the farmer, owed to its indulgent land\n",
      "policy the opportunity of securing free homesteads in the West. The\n",
      "immigrant, landing penniless on these shores, as a result of the same\n",
      "beneficent system, often found himself in a little while with an estate\n",
      "as large as many a baronial domain in the Old World. Under a Republican\n",
      "administration, the union had been saved. To it the veterans of the war\n",
      "could turn with confidence for those rewards of service which the\n",
      "government could bestow: pensions surpassing in liberality anything that\n",
      "the world had ever seen. Under a Republican administration also the\n",
      "great debt had been created in the defense of the union, and to the\n",
      "Republican party every investor in government bonds could look for the\n",
      "full and honorable discharge of the interest and principal. The spoils\n",
      "system, inaugurated by Jacksonian Democracy, in turn placed all the\n",
      "federal offices in Republican hands, furnishing an army of party workers\n",
      "to be counted on for loyal service in every campaign.\n",
      "\n",
      "Of all these things Republican leaders made full and vigorous use,\n",
      "sometimes ascribing to the party, in accordance with ancient political\n",
      "usage, merits and achievements not wholly its own. Particularly was this\n",
      "true in the case of saving the union\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 226\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = None\n",
    "max_train_samples = 0\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = lm_datasets['train']\n",
    "    max_train_samples = len(train_dataset)\n",
    "    if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n",
    "    logger.debug(\"Tokenized training example:\")\n",
    "    logger.debug(tokenizer.decode(train_dataset[0]['input_ids']))\n",
    "\n",
    "eval_dataset = None\n",
    "max_eval_samples = 0\n",
    "if training_args.do_eval:\n",
    "    if \"validation\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = lm_datasets[\"validation\"]\n",
    "    max_eval_samples = len(eval_dataset)\n",
    "    if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n",
    "        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n",
    "    logger.debug(\"Tokenized eval example:\")\n",
    "    logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))\n",
    "    \n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.config.use_cache = False\n",
    "else:\n",
    "    model.config.use_cache = True\n",
    "model.enable_input_require_grads()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "trainer = SavePeftModelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=fault_tolerance_data_collator,\n",
    "    compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    "    if training_args.do_eval and not is_torch_tpu_available()\n",
    "    else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:21:45.591 | INFO     | __main__:<cell line: 2>:3 - *** Train ***\n",
      "2023-06-07 17:21:45.934 | DEBUG    | __main__:<cell line: 2>:4 - Train dataloader example: {'input_ids': tensor([[   814,   3026,   4550,  ...,   1069, 122991,  29129],\n",
      "        [  2524,   2523,  32764,  ...,   1190,  30329, 217349],\n",
      "        [  6323,  24194,  16512,  ...,  22181,   1985,  30167],\n",
      "        [ 27957,   1828,   3483,  ...,   9377,  46039,  14876]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[   814,   3026,   4550,  ...,   1069, 122991,  29129],\n",
      "        [  2524,   2523,  32764,  ...,   1190,  30329, 217349],\n",
      "        [  6323,  24194,  16512,  ...,  22181,   1985,  30167],\n",
      "        [ 27957,   1828,   3483,  ...,   9377,  46039,  14876]])}\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29/29 00:29, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.242700</td>\n",
       "      <td>3.842476</td>\n",
       "      <td>0.340176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.176500</td>\n",
       "      <td>3.801954</td>\n",
       "      <td>0.344868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:22:17.636 | DEBUG    | __main__:<cell line: 2>:10 - Training metrics: {'train_runtime': 31.6734, 'train_samples_per_second': 3.568, 'train_steps_per_second': 0.916, 'total_flos': 217701993676800.0, 'train_loss': 4.207809612668794, 'epoch': 0.51, 'train_samples': 226}\n",
      "2023-06-07 17:22:17.641 | INFO     | __main__:<cell line: 2>:14 - Saving model checkpoint to outputs-pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.51\n",
      "  total_flos               =   202750GF\n",
      "  train_loss               =     4.2078\n",
      "  train_runtime            = 0:00:31.67\n",
      "  train_samples            =        226\n",
      "  train_samples_per_second =      3.568\n",
      "  train_steps_per_second   =      0.916\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "if training_args.do_train:\n",
    "    logger.info(\"*** Train ***\")\n",
    "    logger.debug(f\"Train dataloader example: {list(trainer.get_train_dataloader())[0]}\")\n",
    "    checkpoint = None\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = max_train_samples\n",
    "    logger.debug(f\"Training metrics: {metrics}\")\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n",
    "    save_model(training_args.output_dir, model, tokenizer, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:22:17.970 | INFO     | __main__:<cell line: 2>:3 - *** Evaluate ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 17:22:18.703 | DEBUG    | __main__:<cell line: 2>:12 - Eval metrics: {'eval_loss': 3.7907021045684814, 'eval_accuracy': 0.3447702834799609, 'eval_runtime': 0.7269, 'eval_samples_per_second': 13.757, 'eval_steps_per_second': 4.127, 'epoch': 0.51, 'eval_samples': 10, 'perplexity': 44.28748380746952}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       0.51\n",
      "  eval_accuracy           =     0.3448\n",
      "  eval_loss               =     3.7907\n",
      "  eval_runtime            = 0:00:00.72\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     13.757\n",
      "  eval_steps_per_second   =      4.127\n",
      "  perplexity              =    44.2875\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    metrics[\"eval_samples\"] = max_eval_samples\n",
    "    try:\n",
    "        perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    metrics[\"perplexity\"] = perplexity\n",
    "    logger.debug(f\"Eval metrics: {metrics}\")\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs-pt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "adapter_config.json  \u001B[0m\u001B[01;34mruns\u001B[0m/                    train_results.json\n",
      "adapter_model.bin    special_tokens_map.json  trainer_state.json\n",
      "all_results.json     tokenizer.json           training_args.bin\n",
      "eval_results.json    tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls outputs-pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`scripts/merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
