{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 3: Reward Modeling\n",
    "\n",
    "第三阶段：RM(Reward Model)奖励模型建模，构造人类偏好排序数据集，训练奖励模型，用来对齐人类偏好，主要是\"HHH\"原则，具体是\"helpful, honest, harmless\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 数据集：RM阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置运行环境\n",
    "\n",
    "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "# %cd MedicalGPT\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装库和依赖包：\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers>=4.28.1\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm>=4.47.0\n",
    "peft>=0.3.0\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 咱们开始吧\n",
    "\n",
    "环境配置完成，开始导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 18:06:10.521685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 18:06:10.706128: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-07 18:06:12.446835: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 18:06:12.446953: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 18:06:12.446961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('disk/nlp/MedicalGPT/notebook/run_reward_modeling.ipynb')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author:XuMing(xuming624@qq.com)\n",
    "@description:\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from glob import glob\n",
    "from typing import Any, List, Union\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_int8_training\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    BloomForSequenceClassification,\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizer,\n",
    "    BloomTokenizerFast,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer import TRAINING_ARGS_NAME\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"bloom\": (BloomForSequenceClassification, BloomTokenizerFast),\n",
    "    \"llama\": (LlamaForSequenceClassification, LlamaTokenizer),\n",
    "}\n",
    "\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.json\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/reward/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_type: str = field(\n",
    "        default='bloom',\n",
    "        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n",
    "    )\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"../../../models/bigscience/bloomz-560m\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The tokenizer for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=\"float32\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    device_map: Optional[str] = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_type is None:\n",
    "            raise ValueError(\n",
    "                \"You must specify a valid model_type to run training. Available model types are \" + \", \".join(\n",
    "                    MODEL_CLASSES.keys()))\n",
    "        if self.model_name_or_path is None:\n",
    "            raise ValueError(\"You must specify a valid model_name_or_path to run training.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file_dir: Optional[str] = field(default=\"../data/reward/\", metadata={\"help\": \"The train text data file folder.\"})\n",
    "    validation_file_dir: Optional[str] = field(\n",
    "        default=\"../data/reward/\",\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on text file folder.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(default=256, metadata={\"help\": \"Max length of prompt input text\"})\n",
    "    max_target_length: Optional[int] = field(default=256, metadata={\"help\": \"Max length of output text\"})\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=1000,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=10,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[float] = field(\n",
    "        default=0.05,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PeftArguments(TrainingArguments):\n",
    "    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n",
    "    target_modules: Optional[str] = field(default=\"all\")\n",
    "    lora_rank: Optional[int] = field(default=8)\n",
    "    lora_dropout: Optional[float] = field(default=0.05)\n",
    "    lora_alpha: Optional[float] = field(default=32.0)\n",
    "    modules_to_save: Optional[str] = field(default=None)\n",
    "    peft_path: Optional[str] = field(default=None)\n",
    "    \n",
    "    # notebook 直接写进参数\n",
    "    output_dir: str = field(default='outputs-rm')\n",
    "    do_train: bool = field(default=True)\n",
    "    do_eval: bool = field(default=False) # todo: RM eval 暂时有点问题\n",
    "    fp16: bool = field(default=False)\n",
    "    num_train_epochs: float = field(default=0.5)\n",
    "    learning_rate: float = field(default=2e-5)\n",
    "    warmup_steps: int = field(default=10)\n",
    "    weight_decay: float = field(default=0.01)\n",
    "    logging_strategy: str = field(default='steps')\n",
    "    logging_steps: int = field(default=10)\n",
    "    evaluation_strategy: str = field(default='no')\n",
    "    eval_steps: int = field(default=10)\n",
    "    save_strategy: str = field(default='steps')\n",
    "    save_steps: int = field(default=500)\n",
    "    save_total_limit: int = field(default=3)\n",
    "    seed: int = field(default=42)\n",
    "    per_device_train_batch_size: int = field(default=4)\n",
    "    per_device_eval_batch_size: int = field(default=4)\n",
    "    overwrite_output_dir: bool = field(default=True)\n",
    "    logging_first_step: bool = field(default=True)\n",
    "    report_to: str = field(default=\"tensorboard\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    gradient_checkpointing: bool = field(default=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments()\n",
    "data_args = DataTrainingArguments()\n",
    "training_args = PeftArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 18:06:53.657 | WARNING  | __main__:<cell line: 1>:1 - Model args: ModelArguments(model_type='bloom', model_name_or_path='../../../models/bigscience/bloomz-560m', tokenizer_name_or_path=None, load_in_8bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float32', device_map='auto', trust_remote_code=True)\n",
      "2023-06-07 18:06:53.658 | WARNING  | __main__:<cell line: 2>:2 - Data args: DataTrainingArguments(dataset_name=None, dataset_config_name=None, train_file_dir='../data/reward/', validation_file_dir='../data/reward/', max_source_length=256, max_target_length=256, max_train_samples=1000, max_eval_samples=10, overwrite_cache=False, validation_split_percentage=0.05, preprocessing_num_workers=None)\n",
      "2023-06-07 18:06:53.660 | WARNING  | __main__:<cell line: 3>:3 - Training args: PeftArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=10,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-rm/runs/Jun07_18-06-53_ts-be7e7b4bf1f24d21bb9c3d60ec0c1ae2-launcher,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lora_alpha=32.0,\n",
      "lora_dropout=0.05,\n",
      "lora_rank=8,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "modules_to_save=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=0.5,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=outputs-rm,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "peft_path=None,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-rm,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "target_modules=all,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "use_peft=True,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=10,\n",
      "weight_decay=0.01,\n",
      "xpu_backend=None,\n",
      ")\n",
      "2023-06-07 18:06:53.661 | WARNING  | __main__:<cell line: 4>:4 - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "logger.warning(f\"Model args: {model_args}\")\n",
    "logger.warning(f\"Data args: {data_args}\")\n",
    "logger.warning(f\"Training args: {training_args}\")\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义各函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, references, normalize=True, sample_weight=None):\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(references, predictions, normalize=normalize, sample_weight=sample_weight))\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, _ = eval_preds\n",
    "    # Here, predictions is rewards_chosen and rewards_rejected.\n",
    "    # We want to see how much of the time rewards_chosen > rewards_rejected.\n",
    "    preds = np.argmax(preds, axis=0)\n",
    "    labels = np.zeros(preds.shape)\n",
    "    return accuracy.compute(predictions=preds, references=labels)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    \"\"\"We need to define a special data collator that batches the data in our chosen vs rejected format\"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        features_chosen = []\n",
    "        features_rejected = []\n",
    "        for feature in features:\n",
    "            features_chosen.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_chosen\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_chosen\"],\n",
    "                }\n",
    "            )\n",
    "            features_rejected.append(\n",
    "                {\n",
    "                    \"input_ids\": feature[\"input_ids_rejected\"],\n",
    "                    \"attention_mask\": feature[\"attention_mask_rejected\"],\n",
    "                }\n",
    "            )\n",
    "        batch_chosen = self.tokenizer.pad(\n",
    "            features_chosen,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch_rejected = self.tokenizer.pad(\n",
    "            features_rejected,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids_chosen\": batch_chosen[\"input_ids\"],\n",
    "            \"attention_mask_chosen\": batch_chosen[\"attention_mask\"],\n",
    "            \"input_ids_rejected\": batch_rejected[\"input_ids\"],\n",
    "            \"attention_mask_rejected\": batch_rejected[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer for reward models\n",
    "        Define how to compute the reward loss. Use the InstructGPT pairwise logloss: https://arxiv.org/abs/2203.02155\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards_chosen = model(input_ids=inputs[\"input_ids_chosen\"],\n",
    "                               attention_mask=inputs[\"attention_mask_chosen\"])[0]\n",
    "        rewards_rejected = model(input_ids=inputs[\"input_ids_rejected\"],\n",
    "                                 attention_mask=inputs[\"attention_mask_rejected\"])[0]\n",
    "        loss = -torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_chosen\": rewards_chosen, \"rewards_rejected\": rewards_rejected}\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        \"\"\"Save the LoRA model.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "        self.model.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "def save_model(output_dir, model, tokenizer, args):\n",
    "    \"\"\"Save the model and the tokenizer.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Take care of distributed/parallel training\n",
    "    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    torch.save(args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "\n",
    "\n",
    "class CastOutputToFloat(torch.nn.Sequential):\n",
    "    \"\"\"Cast the output of the model to float\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def find_all_linear_names(peft_model, int4=False, int8=False):\n",
    "    cls = torch.nn.Linear\n",
    "    if int4 or int8:\n",
    "        import bitsandbytes as bnb\n",
    "        if int4:\n",
    "            cls = bnb.nn.Linear4bit\n",
    "        elif int8:\n",
    "            cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            # last layer is not add to lora_module_names\n",
    "            if 'lm_head' in name:\n",
    "                continue\n",
    "            if 'score' in name:\n",
    "                continue\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return sorted(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型和tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of the model checkpoint at ../../../models/bigscience/bloomz-560m were not used when initializing BloomForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing BloomForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BloomForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at ../../../models/bigscience/bloomz-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BloomForSequenceClassification(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1024)\n",
       "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): CastOutputToFloat(\n",
       "    (0): Linear(in_features=1024, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "if not model_args.model_type:\n",
    "    raise ValueError(\"Please specify a model_type, e.g. llama, chatglm, bloom, etc.\")\n",
    "model_class, tokenizer_class = MODEL_CLASSES[model_args.model_type]\n",
    "if model_args.model_name_or_path:\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", None]\n",
    "        else getattr(torch, model_args.torch_dtype)\n",
    "    )\n",
    "    model = model_class.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        num_labels=1,\n",
    "        load_in_8bit=model_args.load_in_8bit,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=model_args.device_map,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "    )\n",
    "    model.score = CastOutputToFloat(model.score)\n",
    "else:\n",
    "    raise ValueError(f\"Error, model_name_or_path is None, RM must be loaded from a pre-trained model\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='../../../models/bigscience/bloomz-560m', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "}\n",
    "tokenizer_name_or_path = model_args.tokenizer_name_or_path\n",
    "if not tokenizer_name_or_path:\n",
    "    tokenizer_name_or_path = model_args.model_name_or_path\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n",
    "# Required for llama\n",
    "if model_args.model_type == \"llama\" and tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": DEFAULT_PAD_TOKEN})\n",
    "    \n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 18:07:04.949 | INFO     | __main__:<cell line: 1>:6 - Init new peft model\n",
      "2023-06-07 18:07:04.951 | INFO     | __main__:<cell line: 1>:13 - Peft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\n",
      "2023-06-07 18:07:04.952 | INFO     | __main__:<cell line: 1>:14 - Peft lora_rank: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3147776 || all params: 562362368 || trainable%: 0.5597415792942959\n"
     ]
    }
   ],
   "source": [
    "if training_args.use_peft:\n",
    "    if training_args.peft_path is not None:\n",
    "        logger.info(f\"Peft from pre-trained model: {training_args.peft_path}\")\n",
    "        model = PeftModel.from_pretrained(model, training_args.peft_path, is_trainable=True)\n",
    "    else:\n",
    "        logger.info(\"Init new peft model\")\n",
    "        target_modules = training_args.target_modules.split(',') if training_args.target_modules else None\n",
    "        if target_modules and 'all' in target_modules:\n",
    "            target_modules = find_all_linear_names(model, int4=False, int8=model_args.load_in_8bit)\n",
    "        modules_to_save = training_args.modules_to_save\n",
    "        if modules_to_save is not None:\n",
    "            modules_to_save = modules_to_save.split(',')\n",
    "        logger.info(f\"Peft target_modules: {target_modules}\")\n",
    "        logger.info(f\"Peft lora_rank: {training_args.lora_rank}\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            target_modules=target_modules,\n",
    "            inference_mode=False,\n",
    "            r=training_args.lora_rank,\n",
    "            lora_alpha=training_args.lora_alpha,\n",
    "            lora_dropout=training_args.lora_dropout,\n",
    "            modules_to_save=modules_to_save)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "    if model_args.load_in_8bit:\n",
    "        model = prepare_model_for_int8_training(model)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    logger.info(\"Full parameters training\")\n",
    "    print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 18:07:09.025 | INFO     | __main__:<cell line: 2>:27 - train files: ../data/reward/test.json\n",
      "2023-06-07 18:07:09.027 | INFO     | __main__:<cell line: 2>:32 - eval files: ../data/reward/test.json\n",
      "Found cached dataset json (/home/flemingxu/.cache/huggingface/datasets/json/default-40481943d2030158/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d203cdab76394144ab271b519d50f932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 18:07:14.055 | INFO     | __main__:<cell line: 53>:53 - Raw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Get reward dataset for tuning the reward model.\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file_dir is not None and os.path.exists(data_args.train_file_dir):\n",
    "        train_data_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + glob(\n",
    "            f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)\n",
    "        logger.info(f\"train files: {', '.join(train_data_files)}\")\n",
    "        data_files[\"train\"] = train_data_files\n",
    "    if data_args.validation_file_dir is not None and os.path.exists(data_args.validation_file_dir):\n",
    "        eval_data_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + glob(\n",
    "            f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)\n",
    "        logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n",
    "        data_files[\"validation\"] = eval_data_files\n",
    "    raw_datasets = load_dataset(\n",
    "        'json',\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            'json',\n",
    "            data_files=data_files,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            'json',\n",
    "            data_files=data_files,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "logger.info(f\"Raw datasets: {raw_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 18:07:14.067 | DEBUG    | __main__:<cell line: 29>:37 - Example train_dataset[0]: {'question': '肛门病变可能是什么疾病的症状?', 'response_chosen': '食管克罗恩病', 'response_rejected': '肛门病变可能与多种不同类型的病症有关。'}\n",
      "Loading cached shuffled indices for dataset at /home/flemingxu/.cache/huggingface/datasets/json/default-40481943d2030158/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-4cbc7d6a2a722e5f.arrow\n",
      "Loading cached processed dataset at /home/flemingxu/.cache/huggingface/datasets/json/default-40481943d2030158/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-9b8af000602773c4.arrow\n",
      "Loading cached processed dataset at /home/flemingxu/.cache/huggingface/datasets/json/default-40481943d2030158/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-92a864269654bf99.arrow\n",
      "2023-06-07 18:07:14.340 | DEBUG    | __main__:<cell line: 29>:51 - Num train_samples: 98\n",
      "2023-06-07 18:07:14.341 | DEBUG    | __main__:<cell line: 29>:52 - Tokenized training example:\n",
      "2023-06-07 18:07:14.343 | DEBUG    | __main__:<cell line: 29>:53 - Question: 为什么会得大脖子病\n",
      "\n",
      "Answer: 甲状腺肿的病因还没完全清楚，情绪、药物、化学物质、放射线、遗传缺损、炎症、自身免疫等因素干扰甲状腺激素的合成、储存与释放，及血中存在刺激甲状腺生长的因子都可引起甲状腺肿。结节性甲状腺肿的CT特点为病灶多发、形态规则、边界清楚、无淋巴结肿大或周围组织受侵改变，肿瘤多呈密度均匀一致的低密度影，但合并囊变、出血、钙化等现象并不少见。弥漫性甲状腺肿二维超声显示甲状腺体积明显增大，形态饱满，包膜完整，边界清晰，回声增强，分布均匀彩色多普勒见甲状腺腺体内血流丰富，甲状腺内血流信号明显增多且呈搏动闪烁亮点“火海征”。单纯性甲状腺肿在早期甲状腺呈弥漫性增大、质软、对称。镜下可见腺泡细胞增生，呈柱状并构成乳头状体突入泡腔，腔内胶质成分少。持续较长时期后，有些泡腔内胶质大量增加，上皮细胞受压呈扁平状。部分腺泡可发生坏死、出血、囊样变性等，滤泡间及小叶间结缔组织增生，集成结节，此时甲状腺呈结节状肿大，质地变硬，形成结节性甲状腺肿。弥漫性甲状腺肿，一般不引起临床症状。但如为异位甲状腺肿或已形成结节性甲状腺肿，生长过大亦可引起压迫症状，如呼吸困难、刺激性干咳、喘鸣、发绀、颈静脉怒张、声音嘶哑等。有时囊内出血可出现某一结节突然增大伴有疼痛，以后又慢慢吸收而缩小。甲状腺功能多正常。在流行地区缺碘严重时，可发生呆小病或幼年型粘液水肿。单纯性甲状腺肿应着重预防，特别是地方性甲状腺肿，食盐中加碘是有效的预防方法。近年应用碘油肌肉注射法亦有良好预防效果。青春期甲状腺肿多可自行消退。对已形成的甲状腺肿不消退者可服甲状腺制剂治疗，有压迫症状或影响外观或生活时，可行手术治疗。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "    num_rows: 98\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets\n",
    "max_length = data_args.max_source_length + data_args.max_target_length\n",
    "\n",
    "def preprocess_reward_function(examples):\n",
    "    \"\"\"\n",
    "    Turn the dataset into pairs of Question + Answer, where input_ids_chosen is the preferred question + answer\n",
    "        and text_rejected is the other.\n",
    "    \"\"\"\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for question, chosen, rejected in zip(examples[\"question\"], examples[\"response_chosen\"],\n",
    "                                          examples[\"response_rejected\"]):\n",
    "        tokenized_chosen = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + chosen)\n",
    "        tokenized_rejected = tokenizer(\"Question: \" + question + \"\\n\\nAnswer: \" + rejected)\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "train_dataset = None\n",
    "max_train_samples = 0\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets['train']\n",
    "    max_train_samples = len(train_dataset)\n",
    "    if data_args.max_train_samples is not None and data_args.max_train_samples > 0:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n",
    "    with training_args.main_process_first(desc=\"Train dataset tokenization\"):\n",
    "        tokenized_dataset = train_dataset.shuffle().map(\n",
    "            preprocess_reward_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=train_dataset.column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "        train_dataset = tokenized_dataset.filter(\n",
    "            lambda x: 0 < len(x['input_ids_rejected']) <= max_length and 0 < len(\n",
    "                x['input_ids_chosen']) <= max_length\n",
    "        )\n",
    "        logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n",
    "        logger.debug(\"Tokenized training example:\")\n",
    "        logger.debug(tokenizer.decode(train_dataset[0]['input_ids_chosen']))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = None\n",
    "max_eval_samples = 0\n",
    "if training_args.do_eval:\n",
    "    with training_args.main_process_first(desc=\"Eval dataset tokenization\"):\n",
    "        if \"validation\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = raw_datasets[\"validation\"]\n",
    "        max_eval_samples = len(eval_dataset)\n",
    "        if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "        logger.debug(f\"Example eval_dataset[0]: {eval_dataset[0]}\")\n",
    "        tokenized_dataset = eval_dataset.map(\n",
    "            preprocess_reward_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=eval_dataset.column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "        eval_dataset = tokenized_dataset.filter(\n",
    "            lambda x: 0 < len(x['input_ids_rejected']) <= max_length and 0 < len(\n",
    "                x['input_ids_chosen']) <= max_length\n",
    "        )\n",
    "        logger.debug(f\"Num eval_samples: {len(eval_dataset)}\")\n",
    "        logger.debug(\"Tokenized eval example:\")\n",
    "        logger.debug(tokenizer.decode(eval_dataset[0]['input_ids_chosen']))\n",
    "\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.config.use_cache = False\n",
    "else:\n",
    "    model.config.use_cache = True\n",
    "model.enable_input_require_grads()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # Keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, max_length=max_length, padding=\"max_length\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "if training_args.do_train:\n",
    "    logger.info(\"*** Train ***\")\n",
    "    logger.debug(f\"Train dataloader example: {list(trainer.get_train_dataloader())[0]}\")\n",
    "    checkpoint = None\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    metrics[\"train_samples\"] = max_train_samples\n",
    "    logger.debug(f\"Training metrics: {metrics}\")\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    logger.info(f\"Saving model checkpoint to {training_args.output_dir}\")\n",
    "    save_model(training_args.output_dir, model, tokenizer, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    metrics[\"eval_samples\"] = max_eval_samples\n",
    "    try:\n",
    "        perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    metrics[\"perplexity\"] = perplexity\n",
    "    logger.debug(f\"Eval metrics: {metrics}\")\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls outputs-rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`\n",
    "- 日志保存在`output_dir/logs`\n",
    "\n",
    "查看运行日志，默认使用的是tensorboard保存日志，启动方式如下：\n",
    "\n",
    "\n",
    "```\n",
    "tensorboard --logdir output_dir/logs --host 0.0.0.0 --port 8009\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38xm",
   "language": "python",
   "name": "py38kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
