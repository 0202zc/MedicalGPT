{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "| Stage 4: Reinforcement Learning | 基于人类反馈的强化学习(RLHF)，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文本   | [scripts/rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/scripts/rl_training.py) | [scripts/run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/scripts/run_rl.sh)    | [notebook/run_rl_training.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/notebook/run_rl_training.ipynb)     | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/notebook/run_rl_training.ipynb)           |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Reinforcement Learning Training\n",
    "\n",
    "第四阶段：RL(Reinforcement Learning)基于人类反馈的强化学习(RLHF)，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型、奖励模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 奖励模型：使用的是`OpenAssistant/reward-model-deberta-v3-large-v2`\n",
    "3. 数据集：RL阶段的数据可以复用SFT的数据集，使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置运行环境\n",
    "\n",
    "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "# %cd MedicalGPT\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装库和依赖包：\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers>=4.28.1\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm>=4.47.0\n",
    "peft>=0.3.0\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/lvwerra/trl\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 咱们开始吧\n",
    "\n",
    "环境配置完成，开始导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:57:23.133400: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 15:57:23.305250: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-07 15:57:25.082512: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 15:57:25.082617: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-07 15:57:25.082626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('disk/nlp/MedicalGPT/notebook/run_rl_training.ipynb')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "/home/flemingxu/disk/py38/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author:XuMing(xuming624@qq.com)\n",
    "@description: Train a model from SFT using PPO\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from loguru import logger\n",
    "from peft import LoraConfig, TaskType\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    BloomForCausalLM,\n",
    "    AutoModel,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    BloomTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    ")\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"FALSE\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n",
    "    \"chatglm\": (AutoModel, AutoTokenizer),\n",
    "    \"llama\": (LlamaForCausalLM, LlamaTokenizer),\n",
    "}\n",
    "\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    \"\"\"\n",
    "    The name of the Casual LM model we wish to fine with PPO\n",
    "    \"\"\"\n",
    "    # Model arguments\n",
    "    model_type: str = field(\n",
    "        default=\"bloom\",\n",
    "        metadata={\"help\": \"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys())}\n",
    "    )\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"bigscience/bloomz-560m\", metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n",
    "    )\n",
    "    reward_model_name_or_path: Optional[str] = field(default=\"OpenAssistant/reward-model-deberta-v3-large-v2\", metadata={\"help\": \"The reward model name\"})\n",
    "\n",
    "    tokenizer_name_or_path: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The tokenizer for weights initialization.\"}\n",
    "    )\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"Whether to load the model in 8bit mode or not.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=\"float16\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
    "                \"dtype will be automatically derived from the model's weights.\"\n",
    "            ),\n",
    "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "        },\n",
    "    )\n",
    "    device_map: Optional[str] = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\"help\": \"Device to map model to. If `auto` is passed, the device will be selected automatically. \"},\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to trust remote code when loading a model from a remote checkpoint.\"},\n",
    "    )\n",
    "    # Dataset arguments\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file_dir: Optional[str] = field(default=\"../data/finetune/\", metadata={\"help\": \"The input jsonl data file folder.\"})\n",
    "    validation_file_dir: Optional[str] = field(default=\"../data/finetune/\", metadata={\"help\": \"The evaluation jsonl file folder.\"}, )\n",
    "    batch_size: Optional[int] = field(default=8, metadata={\"help\": \"Batch size\"})\n",
    "    max_source_length: Optional[int] = field(default=256, metadata={\"help\": \"Max length of prompt input text\"})\n",
    "    max_target_length: Optional[int] = field(default=256, metadata={\"help\": \"Max length of output text\"})\n",
    "    min_target_length: Optional[int] = field(default=4, metadata={\"help\": \"Min length of output text\"})\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=100,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=10,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[float] = field(\n",
    "        default=0.01,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None, metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    # Training arguments\n",
    "    use_peft: bool = field(default=True, metadata={\"help\": \"Whether to use peft\"})\n",
    "    target_modules: Optional[str] = field(default=None)\n",
    "    lora_rank: Optional[int] = field(default=8)\n",
    "    lora_dropout: Optional[float] = field(default=0.05)\n",
    "    lora_alpha: Optional[float] = field(default=32.0)\n",
    "    modules_to_save: Optional[str] = field(default=None)\n",
    "    peft_path: Optional[str] = field(default=None)\n",
    "\n",
    "    do_train: bool = field(default=True, metadata={\"help\": \"Whether to run training.\"})\n",
    "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"})\n",
    "    mini_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"PPO minibatch size\"})\n",
    "    early_stopping: Optional[bool] = field(default=False, metadata={\"help\": \"Whether to early stop\"})\n",
    "    target_kl: Optional[float] = field(default=0.1, metadata={\"help\": \"The kl target for early stopping\"})\n",
    "    reward_baseline: Optional[float] = field(\n",
    "        default=0.0, metadata={\"help\": \"Baseline value that is subtracted from the reward\"},\n",
    "    )\n",
    "    init_kl_coef: Optional[float] = field(\n",
    "        default=0.2, metadata={\"help\": \"Initial KL penalty coefficient (used for adaptive and linear control)\"},\n",
    "    )\n",
    "    adap_kl_ctrl: Optional[bool] = field(default=True, metadata={\"help\": \"Use adaptive KL control, otherwise linear\"})\n",
    "    learning_rate: Optional[float] = field(default=1.5e-5, metadata={\"help\": \"Learning rate\"})\n",
    "    ppo_epochs: Optional[int] = field(default=4, metadata={\"help\": \"the number of ppo epochs\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(\n",
    "        default=1, metadata={\"help\": \"the number of gradient accumulation steps\"}\n",
    "    )\n",
    "    save_steps: Optional[int] = field(default=50, metadata={\"help\": \"X steps to save the model\"})\n",
    "    output_dir: Optional[str] = field(default=\"outputs-rl\", metadata={\"help\": \"n steps to save the model\"})\n",
    "    seed: Optional[int] = field(default=0, metadata={\"help\": \"the seed\"})\n",
    "    max_steps: Optional[int] = field(default=50, metadata={\"help\": \"number of steps to train\"})\n",
    "    log_with: Optional[str] = field(default=\"tensorboard\", metadata={\"help\": \"log with wandb or tensorboard\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScriptArguments(model_type='bloom', model_name_or_path='../../../models/bigscience/bloomz-560m', reward_model_name_or_path='../../../models/OpenAssistant/reward-model-deberta-v3-large-v2', tokenizer_name_or_path=None, load_in_8bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='../data/finetune/', validation_file_dir='../data/finetune/', batch_size=8, max_source_length=256, max_target_length=256, min_target_length=4, max_train_samples=100, max_eval_samples=10, overwrite_cache=False, validation_split_percentage=0.01, preprocessing_num_workers=None, use_peft=True, target_modules=None, lora_rank=8, lora_dropout=0.05, lora_alpha=32.0, modules_to_save=None, peft_path=None, do_train=True, do_eval=False, mini_batch_size=1, early_stopping=False, target_kl=0.1, reward_baseline=0.0, init_kl_coef=0.2, adap_kl_ctrl=True, learning_rate=1.5e-05, ppo_epochs=4, gradient_accumulation_steps=1, save_steps=50, output_dir='outputs-rl', seed=0, max_steps=50, log_with='tensorboard')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = ScriptArguments()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bloom'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_dtype = (\n",
    "    args.torch_dtype\n",
    "    if args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, args.torch_dtype)\n",
    ")\n",
    "torch_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义lora参数量和奖励模型方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_reward_score(reward_model, reward_tokenizer, question, answer, device):\n",
    "    \"\"\"\n",
    "    Get the reward score for a given question and answer pair.\n",
    "    \"\"\"\n",
    "    inputs = reward_tokenizer(question, answer, return_tensors='pt').to(device)\n",
    "    score = reward_model(**inputs).logits[0].cpu().detach()\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bloom.tokenization_bloom_fast.BloomTokenizerFast"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "tokenizer_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomTokenizerFast(name_or_path='../../../models/bigscience/bloomz-560m', vocab_size=250680, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": args.cache_dir,\n",
    "    \"use_fast\": args.use_fast_tokenizer,\n",
    "    \"trust_remote_code\": args.trust_remote_code,\n",
    "}\n",
    "tokenizer_name_or_path = args.tokenizer_name_or_path\n",
    "if not tokenizer_name_or_path:\n",
    "    tokenizer_name_or_path = args.model_name_or_path\n",
    "tokenizer = tokenizer_class.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)\n",
    "# Required for llama\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": DEFAULT_PAD_TOKEN})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:58:10.385 | INFO     | __main__:<cell line: 1>:1 - Load model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 787457 || all params: 560002049 || trainable%: 0.14061680692171896\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Load model\")\n",
    "torch_dtype = (\n",
    "    args.torch_dtype\n",
    "    if args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, args.torch_dtype)\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=args.target_modules,\n",
    "    inference_mode=False,\n",
    "    r=args.lora_rank,\n",
    "    lora_alpha=args.lora_alpha,\n",
    "    lora_dropout=args.lora_dropout,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    cache_dir=args.cache_dir,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=args.device_map,\n",
    "    trust_remote_code=args.trust_remote_code,\n",
    "    peft_config=peft_config if args.use_peft else None,\n",
    ")\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "\n",
    "# Load reward model\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.reward_model_name_or_path,\n",
    "    load_in_8bit=args.load_in_8bit,\n",
    "    cache_dir=args.cache_dir,\n",
    "    torch_dtype=torch_dtype,\n",
    ")\n",
    "reward_model.to(device)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.reward_model_name_or_path, **tokenizer_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:58:38.665 | INFO     | __main__:<cell line: 2>:27 - train files: ../data/finetune/Belle_open_source_1k.json\n",
      "2023-06-07 15:58:38.668 | INFO     | __main__:<cell line: 2>:32 - eval files: ../data/finetune/Belle_open_source_1k.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/flemingxu/.cache/huggingface/datasets/json/default-81868ba195386786/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca39c1237bbf43449f709f0517c19003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216bac7959804f6aa2222ecf921316ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/flemingxu/.cache/huggingface/datasets/json/default-81868ba195386786/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a77aa0ba194b869b0fef197cd485ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:58:40.425 | INFO     | __main__:<cell line: 53>:53 - Raw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Get datasets\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        args.dataset_name,\n",
    "        args.dataset_config_name,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            args.dataset_name,\n",
    "            args.dataset_config_name,\n",
    "            split=f\"train[:{args.validation_split_percentage}%]\",\n",
    "            cache_dir=args.cache_dir,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            args.dataset_name,\n",
    "            args.dataset_config_name,\n",
    "            split=f\"train[{args.validation_split_percentage}%:]\",\n",
    "            cache_dir=args.cache_dir,\n",
    "        )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if args.train_file_dir is not None and os.path.exists(args.train_file_dir):\n",
    "        train_data_files = glob(f'{args.train_file_dir}/**/*.json', recursive=True) + glob(\n",
    "            f'{args.train_file_dir}/**/*.jsonl', recursive=True)\n",
    "        logger.info(f\"train files: {', '.join(train_data_files)}\")\n",
    "        data_files[\"train\"] = train_data_files\n",
    "    if args.validation_file_dir is not None and os.path.exists(args.validation_file_dir):\n",
    "        eval_data_files = glob(f'{args.validation_file_dir}/**/*.json', recursive=True) + glob(\n",
    "            f'{args.validation_file_dir}/**/*.jsonl', recursive=True)\n",
    "        logger.info(f\"eval files: {', '.join(eval_data_files)}\")\n",
    "        data_files[\"validation\"] = eval_data_files\n",
    "    raw_datasets = load_dataset(\n",
    "        'json',\n",
    "        data_files=data_files,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            'json',\n",
    "            data_files=data_files,\n",
    "            split=f\"train[:{args.validation_split_percentage}%]\",\n",
    "            cache_dir=args.cache_dir,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            'json',\n",
    "            data_files=data_files,\n",
    "            split=f\"train[{args.validation_split_percentage}%:]\",\n",
    "            cache_dir=args.cache_dir,\n",
    "        )\n",
    "logger.info(f\"Raw datasets: {raw_datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets\n",
    "max_source_length = args.max_source_length\n",
    "max_target_length = args.max_target_length\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"query\": [],\n",
    "        \"input_ids\": [],\n",
    "    }\n",
    "    for instruction, input in zip(examples['instruction'], examples['input']):\n",
    "        if input:\n",
    "            instruction = instruction + \"\\n\" + input\n",
    "        source = PROMPT_TEMPLATE.format_map({\"instruction\": instruction})\n",
    "        tokenized_question = tokenizer(\n",
    "            source, truncation=True, max_length=max_source_length, padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        new_examples[\"query\"].append(source)\n",
    "        new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
    "\n",
    "    return new_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:58:40.964 | DEBUG    | __main__:<cell line: 4>:12 - Example train_dataset[0]: {'instruction': '为给定的句子生成一个同义句。\\nShe is studying for her final exams.', 'input': '', 'output': 'She is preparing for her last exams.'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:58:41.660 | DEBUG    | __main__:<cell line: 4>:24 - Num train_samples: 100\n",
      "2023-06-07 15:58:41.661 | DEBUG    | __main__:<cell line: 4>:25 - Tokenized training example:\n",
      "2023-06-07 15:58:41.663 | DEBUG    | __main__:<cell line: 4>:26 - [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 111757, 632, 660, 54103, 861, 63808, 267, 20165, 17, 66828, 267, 12427, 861, 156788, 115739, 368, 8821, 6149, 105311, 182924, 29, 189, 38363, 10967, 160455, 355, 42476, 154667, 3105, 54682, 594, 3105, 206537, 814, 22052, 1288, 2067, 2185, 39729, 7818, 1938, 109130, 355, 3574, 39976, 97301, 21556, 11315, 72566, 7882, 420, 16783, 105311, 66673, 29, 210]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "train_dataset = None\n",
    "max_train_samples = 0\n",
    "if args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets['train']\n",
    "    max_train_samples = len(train_dataset)\n",
    "    if args.max_train_samples is not None and args.max_train_samples > 0:\n",
    "        max_train_samples = min(len(train_dataset), args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    logger.debug(f\"Example train_dataset[0]: {train_dataset[0]}\")\n",
    "    tokenized_dataset = train_dataset.shuffle().map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    train_dataset = tokenized_dataset.filter(\n",
    "        lambda x: len(x['input_ids']) > 0\n",
    "    )\n",
    "    logger.debug(f\"Num train_samples: {len(train_dataset)}\")\n",
    "    logger.debug(\"Tokenized training example:\")\n",
    "    logger.debug(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义PPO模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOConfig(model_name='../../../models/bigscience/bloomz-560m', steps=50, learning_rate=1.5e-05, adap_kl_ctrl=True, init_kl_coef=0.2, target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=8, forward_batch_size=None, mini_batch_size=1, gradient_accumulation_steps=1, ppo_epochs=4, remove_unused_columns=True, log_with='tensorboard', tracker_kwargs={}, accelerator_kwargs={'project_dir': 'outputs-rl'}, tracker_project_name='trl', max_grad_norm=None, seed=0, optimize_cuda_cache=True, early_stopping=False, target_kl=0.1, push_to_hub_if_best_kwargs={}, compare_steps=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "output_dir = args.output_dir\n",
    "config = PPOConfig(\n",
    "    steps=args.max_steps,\n",
    "    model_name=args.model_name_or_path,\n",
    "    learning_rate=args.learning_rate,\n",
    "    log_with=args.log_with,\n",
    "    batch_size=args.batch_size,\n",
    "    mini_batch_size=args.mini_batch_size,\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    optimize_cuda_cache=True,\n",
    "    early_stopping=args.early_stopping,\n",
    "    target_kl=args.target_kl,\n",
    "    seed=args.seed,\n",
    "    init_kl_coef=args.init_kl_coef,\n",
    "    adap_kl_ctrl=args.adap_kl_ctrl,\n",
    "    accelerator_kwargs={\"project_dir\": output_dir},\n",
    ")\n",
    "# Set seed before initializing value head for deterministic eval\n",
    "set_seed(config.seed)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trl.trainer.ppo_trainer.PPOTrainer at 0x7f49e9c77160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "trainer = PPOTrainer(\n",
    "    config,\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练之前，先测试各模型是否能正常工作：\n",
    "\n",
    "- 生成模型：测试generate方法的结果是否符合预期\n",
    "- 奖励模型：测试reward模型的reward_score的结果是否符合预期"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 测试生成模型的效果，SFT后的生成模型能否正常生成通顺的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>:who are you?\n",
      "<bot>:I am the one with the most ability\n",
      "\n",
      "<human>:what is the capital of USA?\n",
      "<bot>:Boston\n",
      "\n",
      "<human>:what is the capital of India?\n",
      "<bot>:Shree\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BloomTokenizerFast\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, device_map=\"auto\")\n",
    "ref_tokenizer = BloomTokenizerFast.from_pretrained(args.model_name_or_path)\n",
    "query=[\"who are you?\", \"what is the capital of USA?\", \"what is the capital of India?\"]\n",
    "prompt_query = ['<human>:' + q.strip() + '\\n<bot>:' for q in query]\n",
    "for q in prompt_query:\n",
    "    inputs = ref_tokenizer(q, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device=device)\n",
    "\n",
    "    generate_ids = ref_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=120, \n",
    "        do_sample=True, \n",
    "        top_p=0.85, \n",
    "        temperature=1.0, \n",
    "        repetition_penalty=1.0, \n",
    "        eos_token_id=ref_tokenizer.eos_token_id, \n",
    "        bos_token_id=ref_tokenizer.bos_token_id, \n",
    "        pad_token_id=ref_tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    output = ref_tokenizer.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
    "    print(output)\n",
    "    print()\n",
    "\n",
    "del ref_model\n",
    "del ref_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_cache():\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(10):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 测试 reward 模型的效果，能否正常工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:58:49.475 | INFO     | __main__:<cell line: 6>:6 - good_score: tensor([-1.1055], dtype=torch.float16)\n",
      "2023-06-07 15:58:49.540 | INFO     | __main__:<cell line: 9>:9 - bad_score: tensor([-2.5137], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "q = 'hi'\n",
    "a = 'hello'\n",
    "b = 'torccccc'\n",
    "\n",
    "good_score = get_reward_score(reward_model, reward_tokenizer, q, a, device)\n",
    "logger.info(f\"good_score: {good_score}\")\n",
    "\n",
    "bad_score = get_reward_score(reward_model, reward_tokenizer, q, b, device)\n",
    "logger.info(f\"bad_score: {bad_score}\")\n",
    "assert good_score > bad_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_new_tokens': 256, 'temperature': 1.0, 'repetition_penalty': 1.0, 'top_p': 1.0, 'do_sample': True, 'pad_token_id': 3, 'eos_token_id': 2, 'bos_token_id': 1}\n"
     ]
    }
   ],
   "source": [
    "# These arguments are passed to the `generate` function of the PPOTrainer\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": max_target_length,\n",
    "    \"temperature\": 1.0,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "}\n",
    "print(generation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(save_dir):\n",
    "    \"\"\"Save model\"\"\"\n",
    "    trainer.accelerator.unwrap_model(trainer.model).save_pretrained(save_dir)\n",
    "    trainer.tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 15:58:50.721 | INFO     | __main__:<cell line: 2>:3 - *** Train ***\n",
      "0it [00:00, ?it/s]You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "2023-06-07 15:59:36.909 | DEBUG    | __main__:<cell line: 2>:34 - Step 0/7: reward score:[tensor([-3.6855], dtype=torch.float16), tensor([-2.9043], dtype=torch.float16), tensor([-2.5605], dtype=torch.float16), tensor([-3.1738], dtype=torch.float16), tensor([-3.1387], dtype=torch.float16), tensor([-3.7012], dtype=torch.float16), tensor([-4.6055], dtype=torch.float16), tensor([-2.8008], dtype=torch.float16)]\n",
      "1it [00:46, 46.19s/it]2023-06-07 16:00:19.815 | DEBUG    | __main__:<cell line: 2>:34 - Step 1/7: reward score:[tensor([-0.4875], dtype=torch.float16), tensor([-1.9980], dtype=torch.float16), tensor([-3.1348], dtype=torch.float16), tensor([-2.7383], dtype=torch.float16), tensor([-2.6660], dtype=torch.float16), tensor([-1.3828], dtype=torch.float16), tensor([-1.3242], dtype=torch.float16), tensor([-1.1260], dtype=torch.float16)]\n",
      "2it [01:29, 44.26s/it]2023-06-07 16:01:02.677 | DEBUG    | __main__:<cell line: 2>:34 - Step 2/7: reward score:[tensor([-2.4688], dtype=torch.float16), tensor([-2.4961], dtype=torch.float16), tensor([-0.1594], dtype=torch.float16), tensor([-3.8828], dtype=torch.float16), tensor([-2.3867], dtype=torch.float16), tensor([-1.8750], dtype=torch.float16), tensor([-2.7246], dtype=torch.float16), tensor([-3.1758], dtype=torch.float16)]\n",
      "3it [02:11, 43.62s/it]2023-06-07 16:01:46.789 | DEBUG    | __main__:<cell line: 2>:34 - Step 3/7: reward score:[tensor([-1.4375], dtype=torch.float16), tensor([-2.7676], dtype=torch.float16), tensor([-2.2266], dtype=torch.float16), tensor([-3.1816], dtype=torch.float16), tensor([-0.1998], dtype=torch.float16), tensor([-2.7969], dtype=torch.float16), tensor([-2.7461], dtype=torch.float16), tensor([-0.0408], dtype=torch.float16)]\n",
      "4it [02:56, 43.81s/it]/home/flemingxu/disk/py38/lib/python3.8/site-packages/trl/trainer/ppo_trainer.py:1100: UserWarning: KL divergence is starting to become negative: -2.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "2023-06-07 16:02:33.565 | DEBUG    | __main__:<cell line: 2>:34 - Step 4/7: reward score:[tensor([-1.7188], dtype=torch.float16), tensor([-2.3789], dtype=torch.float16), tensor([-1.7588], dtype=torch.float16), tensor([-2.6973], dtype=torch.float16), tensor([-1.3584], dtype=torch.float16), tensor([-2.8223], dtype=torch.float16), tensor([-0.3298], dtype=torch.float16), tensor([-4.2930], dtype=torch.float16)]\n",
      "5it [03:42, 44.88s/it]/home/flemingxu/disk/py38/lib/python3.8/site-packages/trl/trainer/ppo_trainer.py:1100: UserWarning: KL divergence is starting to become negative: -2.72 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "2023-06-07 16:03:14.778 | DEBUG    | __main__:<cell line: 2>:34 - Step 5/7: reward score:[tensor([-3.1230], dtype=torch.float16), tensor([-4.2812], dtype=torch.float16), tensor([0.3433], dtype=torch.float16), tensor([-1.9287], dtype=torch.float16), tensor([-0.9858], dtype=torch.float16), tensor([-2.3047], dtype=torch.float16), tensor([-2.7129], dtype=torch.float16), tensor([-2.2832], dtype=torch.float16)]\n",
      "6it [04:24, 43.63s/it]/home/flemingxu/disk/py38/lib/python3.8/site-packages/trl/trainer/ppo_trainer.py:1100: UserWarning: KL divergence is starting to become negative: -2.34 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "2023-06-07 16:03:50.897 | DEBUG    | __main__:<cell line: 2>:34 - Step 6/7: reward score:[tensor([-0.7031], dtype=torch.float16), tensor([-0.4863], dtype=torch.float16), tensor([-2.1035], dtype=torch.float16), tensor([-1.5420], dtype=torch.float16), tensor([-1.2031], dtype=torch.float16), tensor([-3.0859], dtype=torch.float16), tensor([-2.9180], dtype=torch.float16), tensor([-3.8457], dtype=torch.float16)]\n",
      "7it [05:00, 42.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "if args.do_train:\n",
    "    logger.info(\"*** Train ***\")\n",
    "    total_steps = config.total_ppo_epochs\n",
    "    for step, batch in tqdm(enumerate(trainer.dataloader)):\n",
    "        if step >= total_steps:\n",
    "            break\n",
    "        question_tensors = batch[\"input_ids\"]\n",
    "        question_tensors = [torch.LongTensor(i).to(device).squeeze(0) for i in question_tensors]\n",
    "        responses = []\n",
    "        response_tensors = []\n",
    "        for q_tensor in question_tensors:\n",
    "            response_tensor = trainer.generate(\n",
    "                q_tensor,\n",
    "                return_prompt=False,\n",
    "                **generation_kwargs,\n",
    "            )\n",
    "            r = tokenizer.batch_decode(response_tensor, skip_special_tokens=True)[0]\n",
    "            responses.append(r)\n",
    "            response_tensors.append(response_tensor.squeeze(0))\n",
    "        batch[\"response\"] = responses\n",
    "\n",
    "        # Compute reward score\n",
    "        score_outputs = [\n",
    "            get_reward_score(reward_model, reward_tokenizer, q, r, device) for q, r in\n",
    "            zip(batch[\"query\"], batch[\"response\"])\n",
    "        ]\n",
    "        rewards = [torch.tensor(float(score) - args.reward_baseline) for score in score_outputs]\n",
    "\n",
    "        # Run PPO step\n",
    "        try:\n",
    "            stats = trainer.step(question_tensors, response_tensors, rewards)\n",
    "            trainer.log_stats(stats, batch, rewards)\n",
    "            logger.debug(f\"Step {step}/{total_steps}: reward score:{score_outputs}\")\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"Failed to log stats for step {step}, because of {e}\")\n",
    "\n",
    "        if step and step % args.save_steps == 0:\n",
    "            save_dir = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "            save_model(save_dir)\n",
    "    # Save final model\n",
    "    save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs-rl'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter_config.json  pytorch_model.bin        tokenizer.json         \u001B[0m\u001B[01;34mtrl\u001B[0m/\n",
      "adapter_model.bin    special_tokens_map.json  tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "%ls outputs-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`scripts/merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/trl`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/trl --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
